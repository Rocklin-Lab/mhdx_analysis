{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "'''\n",
    "Notebook Prerequisites\n",
    "\n",
    "Clustering Results:\n",
    "Ensure you have run mmseqs_dynamic.sh to generate the clustering results. This notebook assumes the script is located in the following directory and executed from the same location:\n",
    "mhdx_analysis/scripts/mmseqs_clustering/mmseqs_dynamic.sh\n",
    "\n",
    "Cooperativity Data:\n",
    "Ensure df_cooperativity.json is available. You can:\n",
    "\n",
    "Refer to DeriveCooperativity.ipynb for instructions on generating this file, or\n",
    "Run scripts/cooperativity/compute_cooperativity.py to create it from your processed data.\n",
    "The file datasets/df_HX_cooperativities.json contains the dataframe used in the paper for training machine learning models. Note: Mutants mutants02, mutants03, and mutants04 must be excluded from the dataframe.\n",
    "\n",
    "Output:\n",
    "The notebook outputs a JSON file containing clustered proteins (fold_r0, fold_r1, fold_r3). The clustering is designed to minimize sequence identity within different folds, reducing data leakage risks during machine learning model training.\n",
    "'''"
   ],
   "id": "8d0b0e1fd133f917"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import numpy as np"
   ],
   "id": "6be0d83cb26572af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.rcParams['xtick.major.width'] = 0.2\n",
    "plt.rcParams['ytick.major.width'] = 0.2\n",
    "plt.rcParams['xtick.minor.width'] = 0.2\n",
    "plt.rcParams['ytick.minor.width'] = 0.2\n",
    "plt.rcParams['xtick.major.size'] = 3\n",
    "plt.rcParams['ytick.major.size'] = 3\n",
    "plt.rcParams['xtick.minor.size'] = 2\n",
    "plt.rcParams['ytick.minor.size'] = 2\n",
    "\n",
    "def plot_pairwise_data(file_path, output_path=None):\n",
    "    # Column names for the TSV file\n",
    "    columns = [\"query\", \"target\", \"identity\", \"alnlen\", \"mismatch\", \"gapopen\",\n",
    "               \"qstart\", \"qend\", \"tstart\", \"tend\", \"evalue\", \"bits\"]\n",
    "    \n",
    "    # Load the TSV file into a pandas dataframe\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\", names=columns)\n",
    "    \n",
    "    # Filter out rows where query == target to avoid self-comparisons\n",
    "    df_filtered = df[df['query'] != df['target']]\n",
    "\n",
    "    # Calculate the statistics for the annotation\n",
    "    num_pairwise_comparisons = len(df)\n",
    "    num_clusters = df['query'].nunique()\n",
    "    num_clusters_more_than_one = df_filtered['query'].nunique()\n",
    "\n",
    "    # Set up the 2x2 grid for plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(7., 5), gridspec_kw={'height_ratios': [1, 1.5]}, constrained_layout=False, dpi=300)\n",
    "    plt.subplots_adjust(wspace=0.35)\n",
    "    \n",
    "    # First row, left: KDE/Histogram of sequence identities\n",
    "    ax1 = axes[0, 0]\n",
    "    sns.histplot(df_filtered['identity'], bins=20, kde=True, ax=ax1, color='dodgerblue', lw=0.5)\n",
    "    ax1.set_xlabel(\"Sequence Identity\", size=6)\n",
    "    ax1.set_ylabel(\"Frequency\", size=6)\n",
    "    ax1.tick_params(axis='both', labelsize=6)\n",
    "    ax1.spines['left'].set_linewidth(0.2)\n",
    "    ax1.spines['bottom'].set_linewidth(0.2)\n",
    "    sns.despine(ax=ax1, top=True)  # Remove top and right spines\n",
    "    \n",
    "    # Add twin y-axis for ECDF plot\n",
    "    ax2 = ax1.twinx()\n",
    "    sns.ecdfplot(df_filtered['identity'], ax=ax2, color='red')  # ECDF plot\n",
    "    ax2.set_ylabel(\"ECDF\", size=6, color='red')\n",
    "    ax2.tick_params(axis='y', labelsize=6, colors='red')\n",
    "    ax2.spines['bottom'].set_linewidth(0.2)\n",
    "    ax2.spines['left'].set_linewidth(0.2)\n",
    "    sns.despine(ax=ax2, top=True)  # Remove top and right spines\n",
    "\n",
    "    # Add the total number of unique sequences, pairwise comparisons, and clusters\n",
    "    ax1.text(0.98, 0.75, \n",
    "             f\"N proteins = {num_pairwise_comparisons}\\nTotal Clusters = {num_clusters}\\nTotal clusters n > 1 = {num_clusters_more_than_one}\",\n",
    "             ha='right', va='top', transform=ax1.transAxes, fontsize=5)\n",
    "\n",
    "    # First row, right: Scatter plot of sequence identity vs. alignment length\n",
    "    sns.scatterplot(x=\"alnlen\", y=\"identity\", data=df_filtered, ax=axes[0, 1], color='dodgerblue', \n",
    "                    edgecolor='black', s=10, alpha=0.6, linewidth=0.2)  # Added alpha and set linewidth to 0.2\n",
    "    axes[0, 1].set_xlabel(\"Alignment Length\", size=6)\n",
    "    axes[0, 1].set_ylabel(\"Sequence Identity\", size=6)\n",
    "    axes[0, 1].tick_params(axis='both', labelsize=6)\n",
    "    axes[0, 1].set_ylim(0.2, 1)  # Adjust y-axis limits\n",
    "    # Remove top and right spines\n",
    "    sns.despine(ax=axes[0, 1], top=True, right=True)\n",
    "    axes[0, 1].spines['left'].set_linewidth(0.2)\n",
    "    axes[0, 1].spines['bottom'].set_linewidth(0.2)\n",
    "\n",
    "    for ax in axes.flatten()[-2:]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Create a merged axis for the second row (spanning the entire width)\n",
    "    ax_full = fig.add_subplot(212)  # This replaces axes[1, 0] and axes[1, 1]\n",
    "\n",
    "    # Box plot of sequence identities per query (sorted by median)\n",
    "    sorted_df = df_filtered.groupby('query')['identity'].median().sort_values(ascending=False).index\n",
    "\n",
    "    # Add the countplot first (so it appears behind the boxplot)\n",
    "    ax_count = ax_full.twinx()\n",
    "    sns.countplot(x=\"query\", data=df_filtered, order=sorted_df, ax=ax_count, color='lightgray', alpha=0.5)\n",
    "    ax_count.set_ylabel(\"Count\", size=6)\n",
    "    ax_count.tick_params(axis='y', labelsize=6)\n",
    "    sns.despine(ax=ax_count, top=True, right=True)  # Remove top and right spines\n",
    "    ax_count.spines['left'].set_linewidth(0.2)\n",
    "    ax_count.spines['bottom'].set_linewidth(0.2)\n",
    "    \n",
    "    # Annotate the counts above the bars\n",
    "    for p in ax_count.patches:\n",
    "        height = p.get_height()\n",
    "        ax_count.annotate(f'{height:.0f}', (p.get_x() + p.get_width() / 2., height),\n",
    "                          ha='center', va='bottom', fontsize=5, color='black')\n",
    "\n",
    "    # Now overlay the boxplot on top of the countplot\n",
    "    sns.boxplot(x=\"query\", y=\"identity\", data=df_filtered, ax=ax_full, order=sorted_df, color='dodgerblue', fliersize=3, linewidth=0.75, \n",
    "                boxprops=dict(edgecolor='black', linewidth=0.2), whiskerprops=dict(color='black', linewidth=0.2), capprops=dict(color='black', linewidth=0.2), \n",
    "                medianprops=dict(color='black', linewidth=0.2), flierprops=dict(marker='o',\n",
    "                                                                                markerfacecolor='dodgerblue', \n",
    "                                                                                markeredgecolor='black', \n",
    "                                                                                markersize=3, markeredgewidth=0.2), zorder=10)\n",
    "\n",
    "    ax_full.set_xlabel(\"\")\n",
    "    ax_full.set_ylabel(\"Sequence Identity\", size=6)\n",
    "    ax_full.tick_params(axis='x', rotation=90, labelsize=6)\n",
    "    ax_full.tick_params(axis='y', labelsize=6)\n",
    "    ax_full.set_ylim(0.2, 1)  # Adjust y-axis limits\n",
    "    # Remove top and right spines\n",
    "    sns.despine(ax=ax_full, top=True, right=True)\n",
    "    ax_full.spines['left'].set_linewidth(0.2)\n",
    "    ax_full.spines['bottom'].set_linewidth(0.2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure if output_path is provided, otherwise display it\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, format='pdf')\n",
    "        print(f\"Plot saved to {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def build_cluster_dataframe(fs):\n",
    "    \"\"\"\n",
    "    Builds a dataframe containing 'target', 'cluster', 'identity', and 'PF' columns\n",
    "    from a list of alignment dataframes.\n",
    "\n",
    "    Parameters:\n",
    "    - fs: List of file paths to the alignment files.\n",
    "\n",
    "    Returns:\n",
    "    - final_df: A pandas DataFrame with the combined data.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store dataframes\n",
    "    df_list = []\n",
    "\n",
    "    # Iterate over each file in 'fs'\n",
    "    for f in fs:\n",
    "        # Extract PF from the file path\n",
    "        pf = f.split('/')[-2]\n",
    "\n",
    "        # Read the alignment dataframe\n",
    "        # Assuming the file is a TSV (tab-separated values)\n",
    "        columns = ['query', 'target', 'identity', 'alnlen', 'mismatch', 'gapopen',\n",
    "                   'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bits']\n",
    "        df = pd.read_csv(f, sep='\\t', names=columns)\n",
    "\n",
    "        # Filter out rows where 'query' == 'target' to avoid self-comparisons (optional)\n",
    "        # df = df[df['query'] != df['target']]\n",
    "\n",
    "        # Assign 'PF' to the dataframe\n",
    "        df['PF'] = pf\n",
    "\n",
    "        # Assign cluster numbers within this PF\n",
    "        # Count the occurrences of each query within this PF\n",
    "        query_counts = df['query'].value_counts()\n",
    "\n",
    "        # Sort the queries by count descending\n",
    "        sorted_queries = query_counts.index.tolist()\n",
    "\n",
    "        # Assign cluster numbers starting from 0 within this PF\n",
    "        cluster_mapping = {query: idx for idx, query in enumerate(sorted_queries)}\n",
    "\n",
    "        # Map the 'query' strings to their cluster numbers\n",
    "        df['cluster'] = df['query'].map(cluster_mapping)\n",
    "\n",
    "        # Extract the necessary columns\n",
    "        df_extracted = df[['target', 'cluster', 'identity', 'PF']].copy()\n",
    "\n",
    "        # Append the dataframe to the list\n",
    "        df_list.append(df_extracted)\n",
    "\n",
    "    # Concatenate all dataframes into one\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    return final_df\n",
    "\n"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fs = glob.glob(\"../scripts/mmseqs_clustering/mmseqs_outputs_dynamic/*/*_pairwise.tsv\")\n",
    "\n",
    "for f in fs:\n",
    "\n",
    "    pf = f.split('/')[-2]\n",
    "\n",
    "    plot_pairwise_data(f, output_path=f'../scripts/mmseqs_clustering/mmseqs_outputs_dynamic/{pf}_mmseqs.pdf')"
   ],
   "id": "81b92f47fde74536"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "l = []\n",
    "for file_path in fs:\n",
    "\n",
    "    # Column names for the TSV file\n",
    "    columns = [\"query\", \"target\", \"identity\", \"alnlen\", \"mismatch\", \"gapopen\",\n",
    "               \"qstart\", \"qend\", \"tstart\", \"tend\", \"evalue\", \"bits\"]\n",
    "    \n",
    "    # Load the TSV file into a pandas dataframe\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\", names=columns)\n",
    "    df['PF'] = file_path.split('/')[-2]\n",
    "    l.append(df)\n",
    "\n",
    "df = pd.concat(l).reset_index(drop=True)"
   ],
   "id": "f810496884ac93a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 1: Assign cluster numbers per PF (family) based on cluster size\n",
    "df['cluster'] = -1  # Initialize cluster column\n",
    "\n",
    "# Group by PF (family) and assign cluster numbers\n",
    "for pf, group in df.groupby('PF'):\n",
    "    # Rank clusters based on their size (number of members in each query cluster)\n",
    "    cluster_counts = group['query'].value_counts()\n",
    "    cluster_ranking = {query: i for i, (query, _) in enumerate(cluster_counts.items())}\n",
    "    \n",
    "    # Assign cluster numbers\n",
    "    df.loc[df['PF'] == pf, 'cluster'] = df.loc[df['PF'] == pf, 'query'].map(cluster_ranking)"
   ],
   "id": "6552af5e7e5e0871"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 2: Create balanced folds for each family\n",
    "num_folds = 5  # We want to divide into 5 groups (0-4)\n",
    "fold_columns = ['folds_r0', 'folds_r1', 'folds_r2']\n",
    "\n",
    "# Initialize fold columns\n",
    "for fold_col in fold_columns:\n",
    "    df[fold_col] = -1  # Initialize fold columns"
   ],
   "id": "ee63c9b2dd3cfe5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to assign clusters to folds and balance member counts\n",
    "def assign_folds(df, fold_col):\n",
    "    for pf, group in df.groupby('PF'):\n",
    "        # Shuffle clusters to introduce randomness\n",
    "        clusters = group['cluster'].unique()\n",
    "        np.random.shuffle(clusters)\n",
    "        \n",
    "        # Assign folds to clusters while balancing member counts\n",
    "        member_counts = {fold: 0 for fold in range(num_folds)}\n",
    "        cluster_fold_map = {}\n",
    "        \n",
    "        for cluster in clusters:\n",
    "            # Get members in the current cluster\n",
    "            cluster_size = len(group[group['cluster'] == cluster])\n",
    "            \n",
    "            # Find the fold with the least members so far\n",
    "            best_fold = min(member_counts, key=member_counts.get)\n",
    "            \n",
    "            # Assign this cluster to the fold with the least members\n",
    "            cluster_fold_map[cluster] = best_fold\n",
    "            member_counts[best_fold] += cluster_size\n",
    "        \n",
    "        # Assign fold numbers to each row based on the cluster fold map\n",
    "        df.loc[(df['PF'] == pf), fold_col] = df.loc[(df['PF'] == pf), 'cluster'].map(cluster_fold_map)\n"
   ],
   "id": "8e4f126630ad77b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Assign folds for each of the three fold columns\n",
    "for fold_col in fold_columns:\n",
    "    assign_folds(df, fold_col)"
   ],
   "id": "e2a09449f117a50c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.drop_duplicates().reset_index(drop=True).to_json('../datasets/ML/MMSEQS/v13/mmseqs_outputs_dynamic/241120_mmseqs_folds_split.json')",
   "id": "7d54f61ae3aa2969"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "df = pd.read_json('../datasets/ML/MMSEQS/v13/mmseqs_outputs_dynamic/241120_mmseqs_folds_split.json')"
   ],
   "id": "21af06c89f7ced2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_hx = pd.read_json(\"df_cooperativity.json\")",
   "id": "51953e362a9e2c00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_path = '../results/mmseqs/241120_mmseqs_folds_split_FULL.json'\n",
    "\n",
    "if not os.path.isdir(os.path.dirname(output_path)):\n",
    "    os.makedirs(os.path.dirname(output_path))\n",
    "\n",
    "pd.merge(df[['target', 'PF', 'cluster', 'folds_r0', 'folds_r1', 'folds_r2']],\n",
    "         df_hx[['name', 'sequence', 'dg_mean', 'cooperativity_model_global', 'cooperativity_model_pf', 'normalized_cooperativity_model_global', 'normalized_cooperativity_model_pf']], \n",
    "         left_on='target', \n",
    "         right_on='name', \n",
    "         how='left')[[\n",
    "    'name', 'sequence', 'PF', 'cluster', 'folds_r0', 'folds_r1', 'folds_r2', 'dg_mean', 'cooperativity_model_global', 'cooperativity_model_pf', 'normalized_cooperativity_model_global', 'normalized_cooperativity_model_pf'\n",
    "         ]].to_json(output_path)"
   ],
   "id": "1715c76a16d9a1d4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
